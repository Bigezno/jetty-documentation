<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC '-//OASIS//DTD DocBook XML V5.0//EN'
'http://www.oasis-open.org/docbook/xml/5.0b5/dtd/docbook.dtd'
>
<section xml:id="high-load" version="5.0" xmlns="http://docbook.org/ns/docbook"
xmlns:xlink="http://www.w3.org/1999/xlink"
xmlns:xi="http://www.w3.org/2001/XInclude"
xmlns:ns5="http://www.w3.org/2000/svg"
xmlns:ns4="http://www.w3.org/1998/Math/MathML"
xmlns:ns3="http://www.w3.org/1999/xhtml"
xmlns:db="http://docbook.org/ns/docbook">
  <title>High Load</title>
  <para>Configuring Jetty for highload, albeit for load testing or for
  production, requires that the operating system, the JVM, jetty, the
  application, the network and the load generation all be tuned.</para>
  <section>
    <title>Load Generation for Load Testing</title>
    <para>The load generation machines must have their OS, JVM etc tuned just
    as much as the server machines.</para>
    <para>The load generation should not be over the local network on the
    server machine, as this has unrealistic performance and latency as well as
    different packet sizes and transport characteristics.</para>
    <para>The load generator should generate a realistic load:</para>
    <variablelist>
      <varlistentry>
        <listitem>A common mistake is that load generators often open
        relatively few connections that are kept totally busy sending as many
        requests as possible over each connection. This causes the measured
        throughput to be limited by request latency (see Lies Damned Lies and
        Benchmarks for an analysis of such an issue.</listitem>
      </varlistentry>
      <varlistentry>
        <listitem>Another common mistake is to use a TCP/IP for a single
        request and to open many many short lived connections. This will often
        result in accept queues filling and limitations due to file descriptor
        and/or port starvation.</listitem>
      </varlistentry>
      <varlistentry>
        <listitem>A load generator should well model the traffic profile from
        the normal clients of the server. For browsers, this if mostly between
        2 and 6 connections that are mostly idle and that are used in sporadic
        bursts with read times in between. The connections are mostly long held
        HTTP/1.1 connections.</listitem>
      </varlistentry>
      <varlistentry>
        <listitem>Load generators should be written in asynchronous programming
        style, so that limited threads does not limit the maximum number of
        users that can be simulated. If the generator is not asynchronous, then
        a thread pool of 2000 may only be able to simulate 500 or less users.
        The Jetty HttpClient is an ideal basis for building a load generator,
        as it is asynchronous and can be used to simulate many thousands of
        connections (see the Cometd Load Tester for a good example of a
        realistic load generator).</listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section>
    <title>Operating System Tuning</title>
    <para>Both the server machine and any load generating machines need to be
    tuned to support many TCP/IP connections and high throughput.</para>
    <section>
      <title>Linux</title>
      <para>Linux does a reasonable job of self configuring TCP/IP, but there
      are a few limits and defaults that that are best increased. These can
      mostly be configured in /etc/security/limits.conf or via sysctl</para>
      <section>
        <title>TCP Buffer Sizes</title>
        <para>These should be increased to at least 16MB for 10G paths and tune
        the autotuning (although buffer bloat now needs to be
        considered).</para>
        <screen>
<![CDATA[
$ sysctl -w net.core.rmem_max=16777216
$ sysctl -w net.core.wmem_max=16777216
$ sysctl -w net.ipv4.tcp_rmem="4096 87380 16777216"
$ sysctl -w net.ipv4.tcp_wmem="4096 16384 16777216"
 ]]>
        </screen>
      </section>
      <section>
        <title>Queue Sizes</title>
        <para>net.core.somaxconn controls the size of the connection listening
        queue. The default value of 128 and if you are running a high-volume
        server and connections are getting refused at a TCP level, then you
        want to increase this. This is a very tweakable setting in such a case.
        Too high and you'll get resource problems as it tries to notify a
        server of a large number of connections and many will remain pending,
        and too low and you'll get refused connections:</para>
        <screen>
<![CDATA[
 $ sysctl -w net.core.somaxconn=4096
 ]]>
        </screen>
        <para>The net.core.netdev_max_backlog controls the size of the incoming
        packet queue for upper-layer (java) processing. The default (2048) may
        be increased and other related parameters (TODO MORE EXPLANATION)
        adjusted with:</para>
        <screen>
<![CDATA[
$ sysctl -w net.core.netdev_max_backlog=16384
$ sysctl -w net.ipv4.tcp_max_syn_backlog=8192
$ sysctl -w net.ipv4.tcp_syncookies=1
 ]]>
        </screen>
      </section>
      <section>
        <title>Ports</title>
        <para>If many outgoing connections are made (eg on load generators),
        then the operating system may run low on ports. Thus it is best to
        increase the port range used and allow reuse of sockets in
        TIME_WAIT:</para>
        <screen>
<![CDATA[
$ sysctl -w net.ipv4.ip_local_port_range="1024 65535"
$ sysctl -w net.ipv4.tcp_tw_recycle=1
]]>
        </screen>
        <section>
          <section>
            <title>File Descriptors</title>
            <para>Busy servers and load generators may run out of file
            descriptors as the system defaults are normally low. These can be
            increased for a specific user in /etc/security/limits.conf:</para>
            <screen>
<![CDATA[
theusername            hard nofile     40000
theusername            soft nofile     40000
]]>
            </screen>
          </section>
          <section>
            <title>Congestion Control</title>
            <para>Linux supports pluggable congestion control algorithms. To
            get a list of congestion control algorithms that are available in
            your kernel run:</para>
            <screen>
<![CDATA[
$ sysctl net.ipv4.tcp_available_congestion_control
]]>
            </screen>
            <para>If cubic and/or htcp are not listed then you will need to
            research the control algorithms for your kernel. You can try
            setting the control to cubic with:</para>
            <screen>
<![CDATA[
$ sysctl -w net.ipv4.tcp_congestion_control=cubic
]]>
            </screen>
          </section>
        </section>
        <section>
          <title>Mac OS</title>
        </section>
        <section>
          <title>Windows</title>
        </section>
      </section>
      <section>
        <title>Network Tuning</title>
        <para>Intermediaries such as nginx can use non persistent HTTP/1.0
        connection. Make sure that persistent HTTP/1.1 connections are
        used.</para>
      </section>
      <section>
        <title>JVM Tuning</title>
        <itemizedlist>
          <listitem>Tune the 
          <link xlink:href="garbage-collection.html#examples">Garbage
          Collection</link></listitem>
          <listitem>Allocate sufficient memory</listitem>
          <listitem>Use the -server option</listitem>
          <listitem>Jetty Tuning</listitem>
        </itemizedlist>
      </section>
      <section>
        <title>Connectors</title>
        <section>
          <title>Acceptors</title>
          <para>The standard rule of thumb for the number of Accepters to
          configure is one per CPU on a given machine.</para>
        </section>
        <section>
          <title>Low Resource Limits</title>
          <para>Must not be configured for less than the number of expected
          connections.</para>
        </section>
        <section>
          <title>Thread Pool</title>
          <para>Configure with mind to limiting memory usage maximum available.
          Typically &gt;50 and &lt;500</para>
        </section>
      </section>
    </section>
  </section>
</section>
