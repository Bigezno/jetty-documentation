<?xml version="1.0" encoding="utf-8"?>
<!--
//  ========================================================================
//  Copyright (c) 1995-2012 Mort Bay Consulting Pty. Ltd.
//  ========================================================================
//  All rights reserved. This program and the accompanying materials
//  are made available under the terms of the Eclipse Public License v1.0
//  and Apache License v2.0 which accompanies this distribution.
//
//      The Eclipse Public License is available at
//      http://www.eclipse.org/legal/epl-v10.html
//
//      The Apache License v2.0 is available at
//      http://www.opensource.org/licenses/apache2.0.php
//
//  You may elect to redistribute this code under either of these licenses.
//  ========================================================================
-->
<section version="5.0"
         xsi:schemaLocation="http://docbook.org/ns/docbook http://www.docbook.org/xml/5.0/xsd/docbook.xsd"
         xml:id="high-load" xmlns="http://docbook.org/ns/docbook"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xmlns:xs="http://www.w3.org/2001/XMLSchema"
         xmlns:xl="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns="http://docbook.org/ns/docbook">
  <title>High Load</title>

  <para>Configuring Jetty for high load, whether for load testing or for
  production, requires that the operating system, the JVM, Jetty, the
  application, the network and the load generation all be tuned.</para>

  <section>
    <title>Load Generation for Load Testing</title>

    <para>The load generation machines must have their OS, JVM, etc., tuned
    just as much as the server machines.</para>

    <para>The load generation should not be over the local network on the
    server machine, as this has unrealistic performance and latency as well as
    different packet sizes and transport characteristics.</para>

    <para>The load generator should generate a realistic load:</para>

    <itemizedlist>
      <listitem>
        <para>A common mistake is that load generators often open relatively
        few connections that are extremely busy sending as many requests as
        possible over each connection. This causes the measured throughput to
        be limited by request latency (see <link
        xl:href="http://blogs.webtide.com/gregw/entry/lies_damned_lies_and_benchmarks&quot;">Lies,
        Damned Lies and Benchmarks</link> for an analysis of such an
        issue).</para>
      </listitem>

      <listitem>
        <para>Another common mistake is to use TCP/IP for a single request,
        and to open many, many short-lived connections. This often results in
        accept queues filling and limitations due to file descriptor and/or
        port starvation.</para>
      </listitem>

      <listitem>
        <para>A load generator should model the traffic profile from the
        normal clients of the server. For browsers, this is often between two
        and six connections that are mostly idle and that are used in sporadic
        bursts with read times in between. The connections are typically long
        held HTTP/1.1 connections.</para>
      </listitem>

      <listitem>
        <para>Load generators should be written in asynchronous programming
        style, so that a limited number of threads does not restrict the
        maximum number of users that can be simulated. If the generator is not
        asynchronous, a thread pool of 2000 may only be able to simulate 500
        or fewer users. The Jetty HttpClient is an ideal choice for building a
        load generator, as it is asynchronous and can simulate many thousands
        of connections (see the Cometd Load Tester for a good example of a
        realistic load generator).</para>
      </listitem>
    </itemizedlist>
  </section>

  <section>
    <title>Operating System Tuning</title>

    <para>Both the server machine and any load generating machines need to be
    tuned to support many TCP/IP connections and high throughput.</para>

    <section>
      <title>Linux</title>

      <para>Linux does a reasonable job of self-configuring TCP/IP, but there
      are a few limits and defaults that you should increase. You can
      configure most of them in <filename>/etc/security/limits.conf</filename>
      or via <code> sysctl</code>.</para>

      <section>
        <title>TCP Buffer Sizes</title>

        <para>You should increase TCP buffer sizes to at least 16MB for 10G
        paths and tune the autotuning (although you now need to consider
        buffer bloat).</para>

        <screen><![CDATA[

$ sysctl -w net.core.rmem_max=16777216
$ sysctl -w net.core.wmem_max=16777216
$ sysctl -w net.ipv4.tcp_rmem="4096 87380 16777216"
$ sysctl -w net.ipv4.tcp_wmem="4096 16384 16777216"
 
        ]]></screen>
      </section>

      <section>
        <title>Queue Sizes</title>

        <para><code>net.core.somaxconn</code> controls the size of the
        connection listening queue. The default value is 128; if you are
        running a high-volume server and connections are getting refused at a
        TCP level, you need to increase this. This is a very tweakable setting
        in such a case: if you set it too high, resource problems occur as it
        tries to notify a server of a large number of connections, and many
        remain pending, but if you set it too low, refused connections
        occur.</para>

        <screen><![CDATA[

 $ sysctl -w net.core.somaxconn=4096
 
        ]]></screen>

        <para>The <code>net.core.netdev_max_backlog</code> controls the size
        of the incoming packet queue for upper-layer (java) processing. The
        default (2048) may be increased and other related parameters (TODO
        MORE EXPLANATION) adjusted with:</para>

        <screen><![CDATA[

$ sysctl -w net.core.netdev_max_backlog=16384
$ sysctl -w net.ipv4.tcp_max_syn_backlog=8192
$ sysctl -w net.ipv4.tcp_syncookies=1
 
        ]]></screen>
      </section>

      <section>
        <title>Ports</title>

        <para>If many outgoing connections are made (for example, on load
        generators), the operating system might run low on ports. Thus it is
        best to increase the port range, and allow reuse of sockets in
        TIME_WAIT:</para>

        <screen><![CDATA[

$ sysctl -w net.ipv4.ip_local_port_range="1024 65535"
$ sysctl -w net.ipv4.tcp_tw_recycle=1

        ]]></screen>

        <section>
          <section>
            <title>File Descriptors</title>

            <para>Busy servers and load generators may run out of file
            descriptors as the system defaults are normally low. These can be
            increased for a specific user in
            <filename>/etc/security/limits.conf</filename>:</para>

            <screen><![CDATA[

theusername            hard nofile     40000
theusername            soft nofile     40000

            ]]></screen>
          </section>

          <section>
            <title>Congestion Control</title>

            <para>Linux supports pluggable congestion control algorithms. To
            get a list of congestion control algorithms that are available in
            your kernel run:</para>

            <screen><![CDATA[

$ sysctl net.ipv4.tcp_available_congestion_control

            ]]></screen>

            <para>If cubic and/or htcp are not listed, you need to research
            the control algorithms for your kernel. You can try setting the
            control to cubic with:</para>

            <screen><![CDATA[

$ sysctl -w net.ipv4.tcp_congestion_control=cubic

            ]]></screen>
          </section>
        </section>

        <section>
          <title>Mac OS</title>

          <para>TBD</para>
        </section>

        <section>
          <title>Windows</title>

          <para>TBD</para>
        </section>
      </section>

      <section>
        <title>Network Tuning</title>

        <para>Intermediaries such as nginx can use a non-persistent HTTP/1.0
        connection. Make sure to use persistent HTTP/1.1 connections.</para>
      </section>

      <section>
        <title>JVM Tuning</title>

        <itemizedlist>
          <listitem>
            <para>Tune the <link
            xl:href="garbage-collection.html#examples">Garbage
            Collection</link></para>
          </listitem>

          <listitem>
            <para>Allocate sufficient memory</para>
          </listitem>

          <listitem>
            <para>Use the -server option</para>
          </listitem>

          <listitem>
            <para>Jetty Tuning</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Connectors</title>

        <section>
          <title>Acceptors</title>

          <para>The standard rule of thumb for the number of Accepters to
          configure is one per CPU on a given machine.</para>
        </section>

        <section>
          <title>Low Resource Limits</title>

          <para>Must not be configured for less than the number of expected
          connections.</para>
        </section>

        <section>
          <title>Thread Pool</title>

          <para>Configure with goal of limiting memory usage maximum
          available. Typically &gt;50 and &lt;500</para>
        </section>
      </section>
    </section>
  </section>
</section>
